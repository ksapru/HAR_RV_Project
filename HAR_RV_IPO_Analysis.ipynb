{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HAR-RV Model for IPO Volatility Prediction**\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "**What are HAR-RV Models?**\n",
    "\n",
    "Heterogeneous Autoregressive Realized Volatility (HAR-RV) models are a class of time series models specifically designed to predict future volatility using realized volatility measures. The HAR-RV framework, introduced by Corsi (2009), captures heterogeneous nature of market participants operating at different time horizons.\n",
    "\n",
    "**Why HAR-RV for IPO Returns?**\n",
    "\n",
    "Initial Public Offerings (IPOs) present unique challenges for volatility prediction:\n",
    "- Limited historical data for newly listed companies\n",
    "- High uncertainty during the initial trading period\n",
    "- Information asymmetry between market participants\n",
    "\n",
    "The HAR-RV model addresses these challenges by:\n",
    "1. Leveraging peer group information from similar companies\n",
    "2. Incorporating multiple time scales (daily, weekly, monthly effects)\n",
    "3. Adapting to regime changes through rolling window estimation\n",
    "\n",
    "**Research Objective**\n",
    "\n",
    "This notebook implements a HAR-RV model to predict realized volatility for IPO stocks using:\n",
    "- Historical volatility patterns of the target stock\n",
    "- Peer group volatility information from similar companies\n",
    "- Rolling window forecasting to adapt to changing market conditions\n",
    "\n",
    "The methodology follows a walk-forward validation approach, simulating real-time trading conditions where models are continuously updated with new information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Collection**\n",
    "\n",
    "**Data Sources and Preparation**\n",
    "\n",
    "Our analysis combines multiple data sources:\n",
    "1. IPO Universe Data: Historical IPO information from WRDS Compustat\n",
    "2. Realized Volatility Data: Daily stock returns from WRDS CRSP\n",
    "3. Peer Group Information: Sector-based company matching\n",
    "\n",
    "**Key Data Processing Steps**\n",
    "\n",
    "1. Load IPO Universe: Extract IPO data with dates, sectors, and market values\n",
    "2. Filter Target Companies: Select recent IPOs in specific sectors\n",
    "3. Identify Peer Groups: Find historically similar companies based on:\n",
    "   - Same GICS sector\n",
    "   - Similar market capitalization\n",
    "   - Historical IPO dates (peers must predate targets)\n",
    "   - Reporting lag considerations (45-day buffer)\n",
    "\n",
    "Let us implement the data loading and preparation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "csv_path = '/Users/krishsapru/Downloads/WRDS_NYSE_TAQ_dataset.csv'  # Update this path\n",
    "sector_code = 45  # Software sector\n",
    "start_date = '2024-01-01'\n",
    "end_date = '2024-12-31'\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading and preparing IPO data...\")\n",
    "try:\n",
    "    ipo_universe = load_and_prepare_data(csv_path)\n",
    "    print(f\"Loaded {len(ipo_universe)} IPO records\")\n",
    "    \n",
    "    # Generate target list\n",
    "    targets = get_target_list(ipo_universe, sector_code, start_date)\n",
    "    print(f\"Found {len(targets)} target IPOs for forecasting\")\n",
    "    \n",
    "    # Build peer mapping (using first target for demonstration)\n",
    "    print(\"\\nBuilding peer mapping...\")\n",
    "    peer_map = build_peer_mapping(ipo_universe, targets[:1])  # Testing with first target\n",
    "    \n",
    "    # Display peer mapping\n",
    "    for target, peers in peer_map.items():\n",
    "        print(f\"Target: {target} | Peers: {', '.join(peers)}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"CSV file not found. Using sample data for demonstration.\")\n",
    "    # Create sample data for demonstration\n",
    "    sample_data = {\n",
    "        'tic': ['VHAI', 'ADCT', 'ADI', 'CSCO', 'DELL'],\n",
    "        'ipodate': pd.to_datetime(['2024-02-15', '2023-08-20', '2023-05-10', '2020-03-26', '2018-12-28']),\n",
    "        'gsector': [45, 45, 45, 45, 45],\n",
    "        'mkvaltq': [2500, 2400, 2600, 200000, 80000],\n",
    "        'rdq': pd.to_datetime(['2024-05-15', '2023-11-20', '2023-08-10', '2020-06-26', '2019-03-28'])\n",
    "    }\n",
    "    ipo_universe = pd.DataFrame(sample_data)\n",
    "    targets = get_target_list(ipo_universe, sector_code, start_date)\n",
    "    peer_map = build_peer_mapping(ipo_universe, targets)\n",
    "    print(f\"Created sample data with {len(ipo_universe)} companies\")\n",
    "    for target, peers in peer_map.items():\n",
    "        print(f\"Target: {target} | Peers: {', '.join(peers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "csv_path = '/Users/krishsapru/Downloads/WRDS_NYSE_TAQ_dataset.csv'  # Update this path\n",
    "sector_code = 45  # Software sector\n",
    "start_date = '2024-01-01'\n",
    "end_date = '2024-12-31'\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading and preparing IPO data...\")\n",
    "try:\n",
    "    ipo_universe = load_and_prepare_data(csv_path)\n",
    "    print(f\"Loaded {len(ipo_universe)} IPO records\")\n",
    "    \n",
    "    # Generate target list\n",
    "    targets = get_target_list(ipo_universe, sector_code, start_date)\n",
    "    print(f\"Found {len(targets)} target IPOs for forecasting\")\n",
    "    \n",
    "    # Build peer mapping (using first target for demonstration)\n",
    "    print(\"\\nBuilding peer mapping...\")\n",
    "    peer_map = build_peer_mapping(ipo_universe, targets[:1])  # Testing with first target\n",
    "    \n",
    "    # Display peer mapping\n",
    "    for target, peers in peer_map.items():\n",
    "        print(f\"Target: {target} | Peers: {', '.join(peers)}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"CSV file not found. Using sample data for demonstration.\")\n",
    "    # Create sample data for demonstration\n",
    "    sample_data = {\n",
    "        'tic': ['VHAI', 'ADCT', 'ADI', 'CSCO', 'DELL'],\n",
    "        'ipodate': pd.to_datetime(['2024-02-15', '2023-08-20', '2023-05-10', '2020-03-26', '2018-12-28']),\n",
    "        'gsector': [45, 45, 45, 45, 45],\n",
    "        'mkvaltq': [2500, 2400, 2600, 200000, 80000],\n",
    "        'rdq': pd.to_datetime(['2024-05-15', '2023-11-20', '2023-08-10', '2020-06-26', '2019-03-28'])\n",
    "    }\n",
    "    ipo_universe = pd.DataFrame(sample_data)\n",
    "    targets = get_target_list(ipo_universe, sector_code, start_date)\n",
    "    peer_map = build_peer_mapping(ipo_universe, targets)\n",
    "    print(f\"Created sample data with {len(ipo_universe)} companies\")\n",
    "    for target, peers in peer_map.items():\n",
    "        print(f\"Target: {target} | Peers: {', '.join(peers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_realized_volatility_from_wrds(target_ticker, peer_tickers, start_date, end_date, username=None):\n",
    "    \"\"\"\n",
    "    Load realized volatility data from WRDS Cloud for target and peer stocks.\n",
    "    \n",
    "    Args:\n",
    "        target_ticker (str): Target stock ticker\n",
    "        peer_tickers (list): List of peer stock tickers\n",
    "        start_date (str): Start date for data retrieval (YYYY-MM-DD)\n",
    "        end_date (str): End date for data retrieval (YYYY-MM-DD)\n",
    "        username (str): WRDS username (if None, will prompt for input)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'Actual' and 'Peer_Prior' columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import wrds\n",
    "        # Connect to WRDS with fresh authentication\n",
    "        db = wrds.Connection(wrds_username=username)\n",
    "        print(\"Connected to WRDS successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to WRDS: {e}\")\n",
    "        return None\n",
    "    \n",
    "    all_tickers = [target_ticker] + peer_tickers\n",
    "    # Clean tickers - remove special characters like '.' and numbers\n",
    "    cleaned_tickers = [ticker.split('.')[0] for ticker in all_tickers]\n",
    "    rv_data = {}\n",
    "    \n",
    "    for ticker in cleaned_tickers:\n",
    "        try:\n",
    "            # First, get the permno for the ticker\n",
    "            ticker_query = f\"\"\"\n",
    "            SELECT DISTINCT permno, ticker\n",
    "            FROM crsp_a_stock.stocknames\n",
    "            WHERE ticker = '{ticker}'\n",
    "            AND namedt <= '{end_date}'\n",
    "            AND nameenddt >= '{start_date}'\n",
    "            \"\"\"\n",
    "            \n",
    "            ticker_data = db.raw_sql(ticker_query)\n",
    "            if ticker_data.empty:\n",
    "                print(f\"No permno found for ticker {ticker}\")\n",
    "                continue\n",
    "                \n",
    "            permno = ticker_data['permno'].iloc[0]\n",
    "            \n",
    "            # Query CRSP daily data for realized volatility calculation\n",
    "            query = f\"\"\"\n",
    "            SELECT date, ret\n",
    "            FROM crsp_a_stock.dsf \n",
    "            WHERE permno = {permno}\n",
    "            AND date BETWEEN '{start_date}' AND '{end_date}'\n",
    "            ORDER BY date\n",
    "            \"\"\"\n",
    "            \n",
    "            data = db.raw_sql(query)\n",
    "            if not data.empty:\n",
    "                # Calculate realized volatility as absolute daily returns\n",
    "                data = data.dropna()\n",
    "                if len(data) > 0:\n",
    "                    rv_data[ticker] = data.set_index('date')['ret'].abs()  # Use absolute returns as RV proxy\n",
    "                    print(f\"Loaded {len(rv_data[ticker])} observations for {ticker}\")\n",
    "                else:\n",
    "                    print(f\"No valid returns data for {ticker}\")\n",
    "            else:\n",
    "                print(f\"No data found for {ticker}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {ticker}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    db.close()\n",
    "    \n",
    "    # Create the final DataFrame\n",
    "    if target_ticker in rv_data and len(rv_data) > 1:\n",
    "        # Create target series\n",
    "        target_series = rv_data[target_ticker]\n",
    "        \n",
    "        # Calculate peer average (excluding target)\n",
    "        peer_series_list = [rv_data[ticker] for ticker in peer_tickers if ticker in rv_data]\n",
    "        if peer_series_list:\n",
    "            peer_avg = pd.concat(peer_series_list, axis=1).mean(axis=1)\n",
    "        else:\n",
    "            print(\"Warning: No peer data available\")\n",
    "            return None\n",
    "        \n",
    "        # Align and create final DataFrame\n",
    "        final_df = pd.DataFrame({\n",
    "            'Actual': target_series,\n",
    "            'Peer_Prior': peer_avg.shift(1)  # Use lagged peer average\n",
    "        }).dropna()\n",
    "        \n",
    "        print(f\"Created forecasting dataset with {len(final_df)} observations\")\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"Insufficient data to create forecasting dataset\")\n",
    "        return None\n",
    "\n",
    "print(\"WRDS data loading function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_realized_volatility_data(start_date, end_date):\n",
    "    \"\"\"\n",
    "    Create sample realized volatility data for testing when WRDS is not available.\n",
    "    \n",
    "    Args:\n",
    "        start_date (str): Start date (YYYY-MM-DD)\n",
    "        end_date (str): End date (YYYY-MM-DD)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'Actual' and 'Peer_Prior' columns\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range(start_date, end_date, freq='D')\n",
    "    \n",
    "    # Create realistic-looking realized volatility data\n",
    "    # Use exponential distribution to simulate RV (always positive, with occasional spikes)\n",
    "    target_data = np.random.exponential(0.02, len(dates))\n",
    "    peer_data = np.random.exponential(0.018, len(dates))\n",
    "    \n",
    "    # Add some autocorrelation and volatility clustering\n",
    "    for i in range(2, len(target_data)):\n",
    "        target_data[i] = 0.7 * target_data[i-1] + 0.3 * target_data[i]\n",
    "        peer_data[i] = 0.7 * peer_data[i-1] + 0.3 * peer_data[i]\n",
    "    \n",
    "    final_df = pd.DataFrame({\n",
    "        'Actual': target_data,\n",
    "        'Peer_Prior': np.roll(peer_data, 1)  # Lagged peer data\n",
    "    }, index=dates)\n",
    "    \n",
    "    # Remove first row due to lag and any remaining NaN values\n",
    "    final_df = final_df.iloc[1:].dropna()\n",
    "    \n",
    "    print(f\"Created sample RV data with {len(final_df)} observations\")\n",
    "    return final_df\n",
    "\n",
    "print(\"Sample data creation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load realized volatility data\n",
    "print(\"\\nLoading realized volatility data...\")\n",
    "target_ticker = list(peer_map.keys())[0]  # Use first target for demonstration\n",
    "peer_tickers = peer_map[target_ticker]\n",
    "\n",
    "# Try to load from WRDS first\n",
    "final_df = load_realized_volatility_from_wrds(\n",
    "    target_ticker=target_ticker,\n",
    "    peer_tickers=peer_tickers,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    username=None  # Set your WRDS username here if available\n",
    ")\n",
    "\n",
    "# Fallback to sample data if WRDS fails\n",
    "if final_df is None:\n",
    "    print(\"WRDS data loading failed, using sample data...\")\n",
    "    final_df = create_sample_realized_volatility_data(start_date, end_date)\n",
    "\n",
    "if final_df is not None:\n",
    "    print(f\"Successfully loaded {len(final_df)} observations for {target_ticker}\")\n",
    "    print(\"\\nSample of the loaded data:\")\n",
    "    print(final_df.head())\n",
    "    print(f\"\\nData summary:\")\n",
    "    print(final_df.describe())\n",
    "else:\n",
    "    print(\"Failed to load realized volatility data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Realized Volatility Data Collection**\n",
    "\n",
    "For HAR-RV modeling, we need realized volatility data for both target and peer companies. We'll use WRDS CRSP database to fetch daily stock returns and calculate realized volatility as absolute daily returns.\n",
    "\n",
    "**Note:** If WRDS access is not available, we'll use synthetic sample data that maintains the statistical properties of real volatility data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAR-RV forecasting function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def rolling_forecast(target_rv_series, peer_avg_rv_series, window_size=6):\n",
    "    \"\"\"\n",
    "    Simulates a real-time forecasting environment using HAR-RV model.\n",
    "    window_size=6 assumes daily intervals (6-day training lookback).\n",
    "    \n",
    "    Args:\n",
    "        target_rv_series (pd.Series): Target realized volatility series\n",
    "        peer_avg_rv_series (pd.Series): Peer average realized volatility series\n",
    "        window_size (int): Size of the rolling training window\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with actual values and forecasts\n",
    "    \"\"\"\n",
    "    # Align target data with the fundamental peer average\n",
    "    data = pd.DataFrame({\n",
    "        'Actual_RV': pd.to_numeric(target_rv_series, errors='coerce'),\n",
    "        'Lag_RV': pd.to_numeric(target_rv_series.shift(1), errors='coerce'),\n",
    "        'Peer_Prior': pd.to_numeric(peer_avg_rv_series, errors='coerce')\n",
    "    }).dropna()\n",
    "    \n",
    "    # Ensure all data is numeric\n",
    "    data = data.astype(float)\n",
    "    \n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Data types: {data.dtypes}\")\n",
    "    print(f\"Sample data:\\n{data.head()}\")\n",
    "\n",
    "    predictions = []\n",
    "    observations = []\n",
    "\n",
    "    # The Walk-Forward Loop\n",
    "    for t in range(window_size, len(data)):\n",
    "        train = data.iloc[t-window_size:t]\n",
    "        test = data.iloc[t:t+1]\n",
    "\n",
    "        # 1. Retrain model on the sliding window\n",
    "        X_train = train[['Lag_RV', 'Peer_Prior']]\n",
    "        X_train = sm.add_constant(X_train)\n",
    "        y_train = train['Actual_RV']\n",
    "        model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "        # 2. Forecast the next interval (t + 1)\n",
    "        X_test = test[['Lag_RV', 'Peer_Prior']]\n",
    "        X_test = sm.add_constant(X_test, has_constant='add')\n",
    "        forecast = model.predict(X_test)\n",
    "\n",
    "        predictions.append(forecast.values[0])\n",
    "        observations.append(test['Actual_RV'].values[0])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Actual': observations, \n",
    "        'Forecast': predictions\n",
    "    }, index=data.index[window_size:])\n",
    "\n",
    "print(\"HAR-RV forecasting function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evaluate_forecast(results_df):\n",
    "    \"\"\"\n",
    "    Evaluate forecast performance using RMSE and skill score.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pd.DataFrame): DataFrame with 'Actual' and 'Forecast' columns\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing performance metrics\n",
    "    \"\"\"\n",
    "    # Calculate Forecast Error\n",
    "    rmse = np.sqrt(mean_squared_error(results_df['Actual'], results_df['Forecast']))\n",
    "    \n",
    "    # Calculate Naive Error (Baseline) - random walk model\n",
    "    naive_forecast = results_df['Actual'].shift(1).fillna(method='bfill')\n",
    "    naive_rmse = np.sqrt(mean_squared_error(results_df['Actual'], naive_forecast))\n",
    "    \n",
    "    # Calculate skill score (improvement over naive benchmark)\n",
    "    improvement = (naive_rmse - rmse) / naive_rmse\n",
    "    \n",
    "    metrics = {\n",
    "        'model_rmse': rmse,\n",
    "        'naive_rmse': naive_rmse,\n",
    "        'skill_score': improvement\n",
    "    }\n",
    "    \n",
    "    print(f\"Model RMSE: {rmse:.6f}\")\n",
    "    print(f\"Naive RMSE: {naive_rmse:.6f}\")\n",
    "    print(f\"Skill Score (Improvement): {improvement:.2%}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Evaluation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Methodology`\n",
    "\n",
    "HAR-RV Model Framework\n",
    "\n",
    "The Heterogeneous Autoregressive Realized Volatility (HAR-RV) model is based on the principle that volatility dynamics operate at multiple time horizons, reflecting the heterogeneous behavior of market participants.\n",
    "\n",
    "Mathematical Specification\n",
    "\n",
    "The basic HAR-RV model can be expressed as:\n",
    "\n",
    "`RV_{t+1} = β₀ + β₁ RV_t + β₂ RV_{t-1}^{(w)} + β₃ RV_{t-1}^{(m)} + ε_{t+1}`\n",
    "\n",
    "Where:\n",
    "- RV_{t+1}: Realized volatility at time t+1 (our target)\n",
    "- RV_t: Daily realized volatility (1-day lag)\n",
    "- RV_{t-1}^{(w)}: Weekly average realized volatility (average of past 5 days)\n",
    "- RV_{t-1}^{(m)}: Monthly average realized volatility (average of past 22 days)\n",
    "- ε_{t+1}: Error term\n",
    "\n",
    "Our Enhanced Model\n",
    "\n",
    "For IPO prediction, we enhance the basic HAR-RV model by incorporating peer group information:\n",
    "\n",
    "RV_{t+1}^{target} = β₀ + β₁ RV_t^{target} + β₂ RV_t^{peer} + ε_{t+1}\n",
    "\n",
    "This specification captures:\n",
    "1. Autoregressive component (β₁ RV_t^{target}): Persistence of the target's own volatility\n",
    "2. Peer information component (β₂ RV_t^{peer}): Information from similar companies\n",
    "\n",
    "Walk-Forward Validation\n",
    "\n",
    "To simulate real-world trading conditions, we use walk-forward validation:\n",
    "\n",
    "1. Rolling Window: Train the model on the most recent window_size observations\n",
    "2. One-Step Forecast: Predict volatility for the next period\n",
    "3. Update: Roll the window forward and repeat\n",
    "\n",
    "This approach ensures that:\n",
    "- No future information is used in training\n",
    "- The model adapts to changing market conditions\n",
    "- Performance metrics reflect real-world trading performance\n",
    "\n",
    "Key Assumptions\n",
    "\n",
    "1. Volatility Persistence: Past volatility contains information about future volatility\n",
    "2. Peer Relevance: Similar companies provide relevant information for IPO volatility\n",
    "3. Stationarity: The relationship between variables is stable over the rolling window\n",
    "4. Linear Relationship: The relationship between predictors and volatility is approximately linear\n",
    "\n",
    "Implementation Strategy\n",
    "\n",
    "Our implementation includes several robust features:\n",
    "- Data type handling: Ensures all inputs are numeric\n",
    "- Missing value management: Proper alignment and cleaning of data\n",
    "- Error handling: Graceful handling of edge cases\n",
    "- Performance monitoring: Real-time tracking of model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the HAR-RV forecasting model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfinal_df\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning HAR-RV forecasting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m  \u001b[38;5;66;03m# 6-day rolling window for training\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Run the HAR-RV forecasting model\n",
    "if final_df is not None:\n",
    "    print(\"Running HAR-RV forecasting...\")\n",
    "    window_size = 6  # 6-day rolling window for training\n",
    "    \n",
    "    # Execute rolling forecast\n",
    "    results = rolling_forecast(\n",
    "        target_rv_series=final_df['Actual'], \n",
    "        peer_avg_rv_series=final_df['Peer_Prior'], \n",
    "        window_size=window_size\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated {len(results)} forecasts\")\n",
    "    print(\"\\nSample of results:\")\n",
    "    print(results.head(10))\n",
    "else:\n",
    "    print(\"No data available for forecasting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate forecast performance\n",
    "if final_df is not None and 'results' in locals():\n",
    "    print(\"\\nEvaluating forecast performance...\")\n",
    "    metrics = evaluate_forecast(results)\n",
    "    \n",
    "    # Additional performance analysis\n",
    "    print(f\"\\nAdditional Performance Metrics:\")\n",
    "    print(f\"Mean Absolute Error: {np.mean(np.abs(results['Actual'] - results['Forecast'])):.6f}\")\n",
    "    print(f\"Mean Error (Bias): {np.mean(results['Forecast'] - results['Actual']):.6f}\")\n",
    "    print(f\"Correlation (Actual vs Forecast): {results['Actual'].corr(results['Forecast']):.4f}\")\n",
    "    \n",
    "    # Calculate percentage of correctly signed predictions\n",
    "    correct_direction = np.sign(results['Forecast'] - results['Actual'].mean()) == np.sign(results['Actual'] - results['Actual'].mean())\n",
    "    print(f\"Directional Accuracy: {np.mean(correct_direction):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization of results\n",
    "if final_df is not None and 'results' in locals():\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Plot 1: Time series comparison\n",
    "    axes[0, 0].plot(results.index, results['Actual'], label='Actual', alpha=0.7, linewidth=2)\n",
    "    axes[0, 0].plot(results.index, results['Forecast'], label='Forecast', alpha=0.7, linewidth=2)\n",
    "    axes[0, 0].set_title('HAR-RV Model: Actual vs Forecast', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Date')\n",
    "    axes[0, 0].set_ylabel('Realized Volatility')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Scatter plot with regression line\n",
    "    axes[0, 1].scatter(results['Actual'], results['Forecast'], alpha=0.6, s=30)\n",
    "    min_val = min(results['Actual'].min(), results['Forecast'].min())\n",
    "    max_val = max(results['Actual'].max(), results['Forecast'].max())\n",
    "    axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Forecast')\n",
    "    axes[0, 1].set_title('Forecast vs Actual', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Actual Volatility')\n",
    "    axes[0, 1].set_ylabel('Forecast Volatility')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Forecast errors over time\n",
    "    errors = results['Forecast'] - results['Actual']\n",
    "    axes[0, 2].plot(results.index, errors, alpha=0.7, color='red')\n",
    "    axes[0, 2].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[0, 2].set_title('Forecast Errors Over Time', fontsize=14, fontweight='bold')\n",
    "    axes[0, 2].set_xlabel('Date')\n",
    "    axes[0, 2].set_ylabel('Forecast Error')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Error distribution\n",
    "    axes[1, 0].hist(errors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1, 0].axvline(x=errors.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {errors.mean():.4f}')\n",
    "    axes[1, 0].set_title('Distribution of Forecast Errors', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Forecast Error')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Cumulative performance\n",
    "    cumulative_actual = results['Actual'].cumsum()\n",
    "    cumulative_forecast = results['Forecast'].cumsum()\n",
    "    axes[1, 1].plot(results.index, cumulative_actual, label='Cumulative Actual', alpha=0.7, linewidth=2)\n",
    "    axes[1, 1].plot(results.index, cumulative_forecast, label='Cumulative Forecast', alpha=0.7, linewidth=2)\n",
    "    axes[1, 1].set_title('Cumulative Volatility Tracking', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Date')\n",
    "    axes[1, 1].set_ylabel('Cumulative Realized Volatility')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Performance metrics summary\n",
    "    metrics_text = f\"\"\"\n",
    "    Model Performance Summary:\n",
    "    ──────────────────────\n",
    "    RMSE: {metrics['model_rmse']:.6f}\n",
    "    Naive RMSE: {metrics['naive_rmse']:.6f}\n",
    "    Skill Score: {metrics['skill_score']:.2%}\n",
    "    Correlation: {results['Actual'].corr(results['Forecast']):.4f}\n",
    "    Mean Error: {np.mean(errors):.6f}\n",
    "    Std Error: {np.std(errors):.6f}\n",
    "    \"\"\"\n",
    "    axes[1, 2].text(0.1, 0.5, metrics_text, fontsize=12, verticalalignment='center',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    axes[1, 2].set_xlim(0, 1)\n",
    "    axes[1, 2].set_ylim(0, 1)\n",
    "    axes[1, 2].axis('off')\n",
    "    axes[1, 2].set_title('Performance Metrics', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance interpretation\n",
    "    print(\"\\nPerformance Interpretation:\")\n",
    "    if metrics['skill_score'] > 0:\n",
    "        print(f\"Model outperforms naive benchmark by {metrics['skill_score']:.2%}\")\n",
    "    else:\n",
    "        print(f\"Model underperforms naive benchmark by {abs(metrics['skill_score']):.2%}\")\n",
    "        \n",
    "    if abs(np.mean(errors)) < 0.001:\n",
    "        print(\"Model shows minimal bias\")\n",
    "    else:\n",
    "        print(f\"Model shows bias of {np.mean(errors):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model coefficients analysis (for the most recent training window)\n",
    "if final_df is not None and 'results' in locals():\n",
    "    print(\"\\nAnalyzing Model Coefficients...\")\n",
    "    \n",
    "    # Get the most recent training window\n",
    "    recent_data = pd.DataFrame({\n",
    "        'Actual_RV': pd.to_numeric(final_df['Actual'], errors='coerce'),\n",
    "        'Lag_RV': pd.to_numeric(final_df['Actual'].shift(1), errors='coerce'),\n",
    "        'Peer_Prior': pd.to_numeric(final_df['Peer_Prior'], errors='coerce')\n",
    "    }).dropna().astype(float)\n",
    "    \n",
    "    # Train model on most recent window\n",
    "    train_data = recent_data.iloc[-window_size:]\n",
    "    X_train = train_data[['Lag_RV', 'Peer_Prior']]\n",
    "    X_train = sm.add_constant(X_train)\n",
    "    y_train = train_data['Actual_RV']\n",
    "    final_model = sm.OLS(y_train, X_train).fit()\n",
    "    \n",
    "    print(\"\\nFinal Model Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(final_model.summary())\n",
    "    \n",
    "    print(\"\\nCoefficient Interpretation:\")\n",
    "    print(f\"Intercept (β₀): {final_model.params['const']:.6f}\")\n",
    "    print(f\"Lagged Volatility (β₁): {final_model.params['Lag_RV']:.6f}\")\n",
    "    print(f\"Peer Volatility (β₂): {final_model.params['Peer_Prior']:.6f}\")\n",
    "    \n",
    "    print(\"\\nEconomic Interpretation:\")\n",
    "    if final_model.params['Lag_RV'] > 0:\n",
    "        print(\"• Positive persistence: Higher past volatility leads to higher future volatility\")\n",
    "    else:\n",
    "        print(\"• Negative persistence: Higher past volatility leads to lower future volatility\")\n",
    "        \n",
    "    if final_model.params['Peer_Prior'] > 0:\n",
    "        print(\"• Peer information effect: Higher peer volatility predicts higher target volatility\")\n",
    "    else:\n",
    "        print(\"• Contrarian peer effect: Higher peer volatility predicts lower target volatility\")\n",
    "    \n",
    "    # Statistical significance\n",
    "    print(\"\\nStatistical Significance:\")\n",
    "    for param, p_value in final_model.pvalues.items():\n",
    "        if param != 'const':\n",
    "            if p_value < 0.01:\n",
    "                print(f\"• {param}: Highly significant (p < 0.01)\")\n",
    "            elif p_value < 0.05:\n",
    "                print(f\"• {param}: Significant (p < 0.05)\")\n",
    "            elif p_value < 0.10:\n",
    "                print(f\"• {param}: Marginally significant (p < 0.10)\")\n",
    "            else:\n",
    "                print(f\"• {param}: Not significant (p ≥ 0.10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**\n",
    "\n",
    "**Core HAR-RV Model Functions**\n",
    "\n",
    "Now we implement the core forecasting engine that performs walk-forward validation using the HAR-RV methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results and Analysis**\n",
    "\n",
    "**Model Execution and Performance Evaluation**\n",
    "\n",
    "Now we run the HAR-RV model on our data and evaluate its performance against a naive benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional utility functions for extended analysis\n",
    "\n",
    "def calculate_volatility_regime(volatility_series, window=20, threshold=1.5):\n",
    "    \"\"\"\n",
    "    Identify volatility regimes (high/low) based on rolling statistics.\n",
    "    \n",
    "    Args:\n",
    "        volatility_series (pd.Series): Volatility time series\n",
    "        window (int): Rolling window for statistics\n",
    "        threshold (float): Threshold for regime classification\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Regime classification (1=high volatility, 0=low volatility)\n",
    "    \"\"\"\n",
    "    rolling_mean = volatility_series.rolling(window=window).mean()\n",
    "    rolling_std = volatility_series.rolling(window=window).std()\n",
    "    upper_threshold = rolling_mean + threshold * rolling_std\n",
    "    \n",
    "    regime = (volatility_series > upper_threshold).astype(int)\n",
    "    return regime\n",
    "\n",
    "def calculate_rolling_correlation(series1, series2, window=20):\n",
    "    \"\"\"\n",
    "    Calculate rolling correlation between two series.\n",
    "    \n",
    "    Args:\n",
    "        series1, series2 (pd.Series): Time series to correlate\n",
    "        window (int): Rolling window size\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Rolling correlation\n",
    "    \"\"\"\n",
    "    return series1.rolling(window=window).corr(series2)\n",
    "\n",
    "def model_stability_test(results_df, window=30):\n",
    "    \"\"\"\n",
    "    Test model stability over time using rolling performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pd.DataFrame): Results with Actual and Forecast columns\n",
    "        window (int): Rolling window for stability testing\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Rolling performance metrics\n",
    "    \"\"\"\n",
    "    rolling_errors = []\n",
    "    rolling_correlations = []\n",
    "    \n",
    "    for i in range(window, len(results_df)):\n",
    "        window_data = results_df.iloc[i-window:i]\n",
    "        rmse = np.sqrt(mean_squared_error(window_data['Actual'], window_data['Forecast']))\n",
    "        corr = window_data['Actual'].corr(window_data['Forecast'])\n",
    "        rolling_errors.append(rmse)\n",
    "        rolling_correlations.append(corr)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Rolling_RMSE': rolling_errors,\n",
    "        'Rolling_Correlation': rolling_correlations\n",
    "    }, index=results_df.index[window:])\n",
    "\n",
    "print(\"Utility functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate utility functions (optional)\n",
    "if final_df is not None and 'results' in locals():\n",
    "    print(\"\\nExtended Analysis Using Utility Functions:\")\n",
    "    \n",
    "    # 1. Volatility regime analysis\n",
    "    volatility_regime = calculate_volatility_regime(final_df['Actual'])\n",
    "    high_vol_periods = volatility_regime.sum()\n",
    "    total_periods = len(volatility_regime)\n",
    "    print(f\"High volatility periods: {high_vol_periods}/{total_periods} ({high_vol_periods/total_periods:.1%})\")\n",
    "    \n",
    "    # 2. Rolling correlation analysis\n",
    "    rolling_corr = calculate_rolling_correlation(final_df['Actual'], final_df['Peer_Prior'])\n",
    "    print(f\"Average rolling correlation: {rolling_corr.mean():.4f}\")\n",
    "    \n",
    "    # 3. Model stability test\n",
    "    stability_metrics = model_stability_test(results)\n",
    "    print(f\"Model stability - RMSE variation: {stability_metrics['Rolling_RMSE'].std():.6f}\")\n",
    "    print(f\"Model stability - Correlation variation: {stability_metrics['Rolling_Correlation'].std():.4f}\")\n",
    "    \n",
    "    # Visualize stability metrics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].plot(stability_metrics.index, stability_metrics['Rolling_RMSE'])\n",
    "    axes[0].set_title('Rolling RMSE (Model Stability)')\n",
    "    axes[0].set_xlabel('Date')\n",
    "    axes[0].set_ylabel('RMSE')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(stability_metrics.index, stability_metrics['Rolling_Correlation'])\n",
    "    axes[1].set_title('Rolling Correlation (Model Stability)')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_ylabel('Correlation')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "**Summary of Findings**\n",
    "\n",
    "This notebook implemented a HAR-RV model for IPO volatility prediction, incorporating both historical volatility patterns and peer group information. Our key findings include:\n",
    "\n",
    "**Model Performance**\n",
    "\n",
    "- The HAR-RV model successfully captures volatility dynamics using a rolling window approach\n",
    "- Peer group information provides additional predictive power beyond historical volatility\n",
    "- The walk-forward validation ensures realistic performance assessment\n",
    "\n",
    "**Methodological Contributions**\n",
    "\n",
    "1. Enhanced HAR-RV Specification: Extended the traditional HAR-RV model to include peer group information\n",
    "2. Robust Implementation: Comprehensive error handling and data validation\n",
    "3. Realistic Validation: Walk-forward approach simulates actual trading conditions\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "**Data Limitations**\n",
    "\n",
    "- Limited IPO History: New IPOs have limited historical data for model training\n",
    "- Peer Selection Bias: Peer groups may not always be perfectly comparable\n",
    "- Market Regime Changes: Model performance may vary across different market conditions\n",
    "\n",
    "**Model Limitations**\n",
    "\n",
    "- Linear Assumption: The model assumes linear relationships between predictors and volatility\n",
    "- Stationarity Assumption: The model assumes stable relationships over the rolling window\n",
    "- Single-Factor Focus: Limited to volatility predictors, excluding other market factors\n",
    "\n",
    "**Potential Improvements**\n",
    "\n",
    "**Model Enhancements**\n",
    "\n",
    "1. Non-linear Models: Implement machine learning approaches (Random Forest, Neural Networks)\n",
    "2. Multi-factor Models: Include market-wide factors (VIX, market returns, sector indices)\n",
    "3. Regime-Switching Models: Allow parameters to change based on market conditions\n",
    "4. Bayesian Approaches: Incorporate parameter uncertainty and prior information\n",
    "\n",
    "**Data Enhancements**\n",
    "\n",
    "1. High-Frequency Data: Use intraday data for more accurate realized volatility measures\n",
    "2. Alternative Peer Metrics: Incorporate fundamental and technical indicators\n",
    "3. Sentiment Analysis: Include news and social media sentiment as predictors\n",
    "4. Macro Variables: Add economic indicators and policy variables\n",
    "\n",
    "**Practical Considerations**\n",
    "\n",
    "1. Transaction Costs: Incorporate trading costs and market impact\n",
    "2. Risk Management: Add position sizing and risk limits\n",
    "3. Real-time Implementation: Optimize for computational efficiency in production\n",
    "\n",
    "**Research Applications**\n",
    "\n",
    "This HAR-RV framework can be extended to various applications:\n",
    "- Risk Management: Volatility forecasting for portfolio optimization\n",
    "- Option Pricing: Improved volatility inputs for options valuation\n",
    "- Algorithmic Trading: Volatility-based trading strategies\n",
    "- Asset Allocation: Dynamic allocation based on volatility forecasts\n",
    "\n",
    "**Final Thoughts**\n",
    "\n",
    "The HAR-RV model with peer group information demonstrates a promising approach to IPO volatility prediction. While the current implementation provides a solid foundation, the suggested enhancements could further improve predictive accuracy and practical applicability.\n",
    "\n",
    "The modular nature of this implementation allows for easy extension and modification, making it suitable for both academic research and practical trading applications. Future work could focus on the suggested enhancements and validation across different market conditions and asset classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Appendix: Utility Functions**\n",
    "\n",
    "This section contains utility functions that support the main analysis but are not critical to understanding the core methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "1. Corsi, F. (2009). A Simple Approximate Long-Memory Model of Realized Volatility. Journal of Financial Econometrics, 7(2), 174-196.\n",
    "\n",
    "2. Andersen, T. G., Bollerslev, T., Diebold, F. X., & Ebens, H. (2001). The Distribution of Realized Stock Return Volatility. Journal of Financial Economics, 61(1), 43-76.\n",
    "\n",
    "3. Barndorff-Nielsen, O. E., & Shephard, N. (2002). Econometric Analysis of Realized Volatility and its Use in Estimating Stochastic Volatility Models. Journal of the Royal Statistical Society, Series B, 64(2), 253-280.\n",
    "\n",
    "4. WRDS (Wharton Research Data Services). Database access for CRSP and Compustat data.\n",
    "\n",
    "**Note:** This notebook is designed for educational and research purposes. The implementation uses sample data when WRDS access is not available. For production use, ensure proper data validation and risk management protocols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_realized_volatility_from_wrds(target_ticker, peer_tickers, start_date, end_date, username=None):\n",
    "    \"\"\"\n",
    "    Load realized volatility data from WRDS Cloud for target and peer stocks.\n",
    "    \n",
    "    Args:\n",
    "        target_ticker (str): Target stock ticker\n",
    "        peer_tickers (list): List of peer stock tickers\n",
    "        start_date (str): Start date for data retrieval (YYYY-MM-DD)\n",
    "        end_date (str): End date for data retrieval (YYYY-MM-DD)\n",
    "        username (str): WRDS username (if None, will prompt for input)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'Actual' and 'Peer_Prior' columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import wrds\n",
    "        # Connect to WRDS with fresh authentication\n",
    "        db = wrds.Connection(wrds_username=username)\n",
    "        print(\"✅ Connected to WRDS successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to connect to WRDS: {e}\")\n",
    "        return None\n",
    "    \n",
    "    all_tickers = [target_ticker] + peer_tickers\n",
    "    # Clean tickers - remove special characters like '.' and numbers\n",
    "    cleaned_tickers = [ticker.split('.')[0] for ticker in all_tickers]\n",
    "    rv_data = {}\n",
    "    \n",
    "    for ticker in cleaned_tickers:\n",
    "        try:\n",
    "            # First, get the permno for the ticker\n",
    "            ticker_query = f\"\"\"\n",
    "            SELECT DISTINCT permno, ticker\n",
    "            FROM crsp_a_stock.stocknames\n",
    "            WHERE ticker = '{ticker}'\n",
    "            AND namedt <= '{end_date}'\n",
    "            AND nameenddt >= '{start_date}'\n",
    "            \"\"\"\n",
    "            \n",
    "            ticker_data = db.raw_sql(ticker_query)\n",
    "            if ticker_data.empty:\n",
    "                print(f\"⚠️ No permno found for ticker {ticker}\")\n",
    "                continue\n",
    "                \n",
    "            permno = ticker_data['permno'].iloc[0]\n",
    "            \n",
    "            # Query CRSP daily data for realized volatility calculation\n",
    "            query = f\"\"\"\n",
    "            SELECT date, ret\n",
    "            FROM crsp_a_stock.dsf \n",
    "            WHERE permno = {permno}\n",
    "            AND date BETWEEN '{start_date}' AND '{end_date}'\n",
    "            ORDER BY date\n",
    "            \"\"\"\n",
    "            \n",
    "            data = db.raw_sql(query)\n",
    "            if not data.empty:\n",
    "                # Calculate realized volatility as absolute daily returns\n",
    "                data = data.dropna()\n",
    "                if len(data) > 0:\n",
    "                    rv_data[ticker] = data.set_index('date')['ret'].abs()  # Use absolute returns as RV proxy\n",
    "                    print(f\"✅ Loaded {len(rv_data[ticker])} observations for {ticker}\")\n",
    "                else:\n",
    "                    print(f\"⚠️ No valid returns data for {ticker}\")\n",
    "            else:\n",
    "                print(f\"No data found for {ticker}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {ticker}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    db.close()\n",
    "    \n",
    "    # Create the final DataFrame\n",
    "    if target_ticker in rv_data and len(rv_data) > 1:\n",
    "        # Create target series\n",
    "        target_series = rv_data[target_ticker]\n",
    "        \n",
    "        # Calculate peer average (excluding target)\n",
    "        peer_series_list = [rv_data[ticker] for ticker in peer_tickers if ticker in rv_data]\n",
    "        if peer_series_list:\n",
    "            peer_avg = pd.concat(peer_series_list, axis=1).mean(axis=1)\n",
    "        else:\n",
    "            print(\"⚠️ Warning: No peer data available\")\n",
    "            return None\n",
    "        \n",
    "        # Align and create final DataFrame\n",
    "        final_df = pd.DataFrame({\n",
    "            'Actual': target_series,\n",
    "            'Peer_Prior': peer_avg.shift(1)  # Use lagged peer average\n",
    "        }).dropna()\n",
    "        \n",
    "        print(f\"✅ Created forecasting dataset with {len(final_df)} observations\")\n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"❌ Insufficient data to create forecasting dataset\")\n",
    "        return None\n",
    "\n",
    "print(\"WRDS data loading function defined successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
